<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Publications | Chulhee “Charlie” Yun</title><link>https://chulheeyun.github.io/publication/</link><atom:link href="https://chulheeyun.github.io/publication/index.xml" rel="self" type="application/rss+xml"/><description>Publications</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Wed, 10 Dec 2025 00:00:00 +0000</lastBuildDate><image><url>https://chulheeyun.github.io/media/icon_huab7a42417b42d6935dd5e37419205d92_14830_512x512_fill_lanczos_center_3.png</url><title>Publications</title><link>https://chulheeyun.github.io/publication/</link></image><item><title>Implicit Bias of Per-sample Adam on Separable Data: Departure from the Full-batch Regime</title><link>https://chulheeyun.github.io/publication/baek2026implicit/</link><pubDate>Wed, 10 Dec 2025 00:00:00 +0000</pubDate><guid>https://chulheeyun.github.io/publication/baek2026implicit/</guid><description/></item><item><title>Implicit Bias and Loss of Plasticity in Matrix Completion: Depth Promotes Low-Rank Solutions</title><link>https://chulheeyun.github.io/publication/shin2025implicit/</link><pubDate>Tue, 10 Jun 2025 03:00:00 +0000</pubDate><guid>https://chulheeyun.github.io/publication/shin2025implicit/</guid><description/></item><item><title>The Cost of Robustness: Tighter Bounds on Parameter Complexity for Robust Memorization in ReLU Nets</title><link>https://chulheeyun.github.io/publication/kim2025cost/</link><pubDate>Tue, 10 Jun 2025 02:00:00 +0000</pubDate><guid>https://chulheeyun.github.io/publication/kim2025cost/</guid><description/></item><item><title>From Linear to Nonlinear: Provable Weak-to-Strong Generalization through Feature Learning</title><link>https://chulheeyun.github.io/publication/oh2025linear/</link><pubDate>Tue, 10 Jun 2025 01:00:00 +0000</pubDate><guid>https://chulheeyun.github.io/publication/oh2025linear/</guid><description/></item><item><title>Through the River: Understanding the Benefit of Schedule-Free Methods for Language Model Training</title><link>https://chulheeyun.github.io/publication/song2025through/</link><pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate><guid>https://chulheeyun.github.io/publication/song2025through/</guid><description/></item><item><title>Provable Benefit of Random Permutations over Uniform Sampling in Stochastic Coordinate Descent</title><link>https://chulheeyun.github.io/publication/kim2025provable/</link><pubDate>Thu, 01 May 2025 02:00:00 +0000</pubDate><guid>https://chulheeyun.github.io/publication/kim2025provable/</guid><description/></item><item><title>Understanding Sharpness Dynamics in NN Training with a Minimalist Example: The Effects of Dataset Difficulty, Depth, Stochasticity, and More</title><link>https://chulheeyun.github.io/publication/yoo2025understanding/</link><pubDate>Thu, 01 May 2025 01:00:00 +0000</pubDate><guid>https://chulheeyun.github.io/publication/yoo2025understanding/</guid><description/></item><item><title>Incremental Gradient Descent with Small Epoch Counts is Surprisingly Slow on Ill-Conditioned Problems</title><link>https://chulheeyun.github.io/publication/kim2025incremental/</link><pubDate>Thu, 01 May 2025 00:00:00 +0000</pubDate><guid>https://chulheeyun.github.io/publication/kim2025incremental/</guid><description/></item><item><title>Lightweight Dataset Pruning without Full Training via Example Difficulty and Prediction Uncertainty</title><link>https://chulheeyun.github.io/publication/cho2025lightweight/</link><pubDate>Mon, 10 Feb 2025 00:00:00 +0000</pubDate><guid>https://chulheeyun.github.io/publication/cho2025lightweight/</guid><description/></item><item><title>Convergence and Implicit Bias of Gradient Descent on Continual Linear Classification</title><link>https://chulheeyun.github.io/publication/jung2025convergence/</link><pubDate>Wed, 22 Jan 2025 02:00:00 +0000</pubDate><guid>https://chulheeyun.github.io/publication/jung2025convergence/</guid><description/></item><item><title>Parameter Expanded Stochastic Gradient Markov Chain Monte Carlo</title><link>https://chulheeyun.github.io/publication/kim2025parameter/</link><pubDate>Wed, 22 Jan 2025 02:00:00 +0000</pubDate><guid>https://chulheeyun.github.io/publication/kim2025parameter/</guid><description/></item><item><title>Arithmetic Transformers Can Length-Generalize in Both Operand Length and Count</title><link>https://chulheeyun.github.io/publication/cho2024arithmetic/</link><pubDate>Wed, 22 Jan 2025 01:00:00 +0000</pubDate><guid>https://chulheeyun.github.io/publication/cho2024arithmetic/</guid><description/></item><item><title>Does SGD really happen in tiny subspaces?</title><link>https://chulheeyun.github.io/publication/song2024does/</link><pubDate>Wed, 22 Jan 2025 00:00:00 +0000</pubDate><guid>https://chulheeyun.github.io/publication/song2024does/</guid><description/></item><item><title>Stochastic Extragradient with Flip-Flop Shuffling &amp; Anchoring: Provable Improvements</title><link>https://chulheeyun.github.io/publication/chae2024stochastic/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://chulheeyun.github.io/publication/chae2024stochastic/</guid><description/></item><item><title>DASH: Warm-Starting Neural Network Training in Stationary Settings without Loss of Plasticity</title><link>https://chulheeyun.github.io/publication/shin2024dash/</link><pubDate>Wed, 19 Jun 2024 00:00:00 +0000</pubDate><guid>https://chulheeyun.github.io/publication/shin2024dash/</guid><description/></item><item><title>Provable Benefit of Cutout and CutMix for Feature Learning</title><link>https://chulheeyun.github.io/publication/oh2024provable/</link><pubDate>Mon, 17 Jun 2024 00:02:00 +0000</pubDate><guid>https://chulheeyun.github.io/publication/oh2024provable/</guid><description/></item><item><title>Position Coupling: Improving Length Generalization of Arithmetic Transformers Using Task Structure</title><link>https://chulheeyun.github.io/publication/cho2024position/</link><pubDate>Fri, 31 May 2024 00:00:00 +0000</pubDate><guid>https://chulheeyun.github.io/publication/cho2024position/</guid><description/></item><item><title>Gradient Descent with Polyak's Momentum Finds Flatter Minima via Large Catapults</title><link>https://chulheeyun.github.io/publication/phunyaphibarn2023large/</link><pubDate>Fri, 24 May 2024 00:00:00 +0000</pubDate><guid>https://chulheeyun.github.io/publication/phunyaphibarn2023large/</guid><description/></item><item><title>Fundamental Benefit of Alternating Updates in Minimax Optimization</title><link>https://chulheeyun.github.io/publication/lee2024fundamental/</link><pubDate>Fri, 16 Feb 2024 00:00:00 +0000</pubDate><guid>https://chulheeyun.github.io/publication/lee2024fundamental/</guid><description/></item><item><title>Linear attention is (maybe) all you need (to understand transformer optimization)</title><link>https://chulheeyun.github.io/publication/ahn2023linear/</link><pubDate>Mon, 02 Oct 2023 00:00:00 +0000</pubDate><guid>https://chulheeyun.github.io/publication/ahn2023linear/</guid><description/></item><item><title>Fair Streaming Principal Component Analysis: Statistical and Algorithmic Viewpoint</title><link>https://chulheeyun.github.io/publication/lee2023fair/</link><pubDate>Fri, 22 Sep 2023 00:00:00 +0000</pubDate><guid>https://chulheeyun.github.io/publication/lee2023fair/</guid><description/></item><item><title>Trajectory Alignment: Understanding the Edge of Stability Phenomenon via Bifurcation Theory</title><link>https://chulheeyun.github.io/publication/song2023trajectory/</link><pubDate>Mon, 10 Jul 2023 00:00:00 +0000</pubDate><guid>https://chulheeyun.github.io/publication/song2023trajectory/</guid><description/></item><item><title>PLASTIC: Improving Input and Label Plasticity for Sample Efficient Reinforcement Learning</title><link>https://chulheeyun.github.io/publication/lee2023enhancing/</link><pubDate>Sun, 25 Jun 2023 00:00:00 +0000</pubDate><guid>https://chulheeyun.github.io/publication/lee2023enhancing/</guid><description/></item><item><title>Practical Sharpness-Aware Minimization Cannot Converge All the Way to Optima</title><link>https://chulheeyun.github.io/publication/si2023practical/</link><pubDate>Thu, 15 Jun 2023 00:00:00 +0000</pubDate><guid>https://chulheeyun.github.io/publication/si2023practical/</guid><description/></item><item><title>Provable Benefit of Mixup for Finding Optimal Decision Boundaries</title><link>https://chulheeyun.github.io/publication/oh2023provable/</link><pubDate>Tue, 25 Apr 2023 00:00:00 +0000</pubDate><guid>https://chulheeyun.github.io/publication/oh2023provable/</guid><description/></item><item><title>Tighter Lower Bounds for Shuffling SGD: Random Permutations and Beyond</title><link>https://chulheeyun.github.io/publication/cha2023tighter/</link><pubDate>Tue, 14 Mar 2023 00:00:00 +0000</pubDate><guid>https://chulheeyun.github.io/publication/cha2023tighter/</guid><description/></item><item><title>On the Training Instability of Shuffling SGD with Batch Normalization</title><link>https://chulheeyun.github.io/publication/wu2023training/</link><pubDate>Fri, 24 Feb 2023 00:00:00 +0000</pubDate><guid>https://chulheeyun.github.io/publication/wu2023training/</guid><description/></item><item><title>SGDA with shuffling: faster convergence for nonconvex-PŁ minimax optimization</title><link>https://chulheeyun.github.io/publication/cho2023sgda/</link><pubDate>Wed, 12 Oct 2022 00:00:00 +0000</pubDate><guid>https://chulheeyun.github.io/publication/cho2023sgda/</guid><description/></item><item><title>Minibatch vs Local SGD with Shuffling: Tight Convergence Bounds and Beyond</title><link>https://chulheeyun.github.io/publication/yun2022minibatch/</link><pubDate>Fri, 01 Apr 2022 00:00:00 +0000</pubDate><guid>https://chulheeyun.github.io/publication/yun2022minibatch/</guid><description/></item><item><title>Open Problem: Can Single-Shuffle SGD be Better than Reshuffling SGD and GD?</title><link>https://chulheeyun.github.io/publication/yun2021can/</link><pubDate>Mon, 02 Aug 2021 00:00:00 +0000</pubDate><guid>https://chulheeyun.github.io/publication/yun2021can/</guid><description/></item><item><title>Provable Memorization via Deep Neural Networks using Sub-linear Parameters</title><link>https://chulheeyun.github.io/publication/park2021provable/</link><pubDate>Sun, 01 Aug 2021 00:00:00 +0000</pubDate><guid>https://chulheeyun.github.io/publication/park2021provable/</guid><description/></item><item><title>A Unifying View on Implicit Bias in Training Linear Neural Networks</title><link>https://chulheeyun.github.io/publication/yun2021unifying/</link><pubDate>Sun, 02 May 2021 00:00:00 +0000</pubDate><guid>https://chulheeyun.github.io/publication/yun2021unifying/</guid><description/></item><item><title>Minimum Width for Universal Approximation</title><link>https://chulheeyun.github.io/publication/park2021minimum/</link><pubDate>Sat, 01 May 2021 00:00:00 +0000</pubDate><guid>https://chulheeyun.github.io/publication/park2021minimum/</guid><description/></item><item><title>SGD with shuffling: optimal rates without component convexity and large epoch requirements</title><link>https://chulheeyun.github.io/publication/ahn2020sgd/</link><pubDate>Wed, 02 Dec 2020 00:00:00 +0000</pubDate><guid>https://chulheeyun.github.io/publication/ahn2020sgd/</guid><description/></item><item><title>$O(n)$ Connections are Expressive Enough: Universal Approximability of Sparse Transformers</title><link>https://chulheeyun.github.io/publication/yun2020n/</link><pubDate>Tue, 01 Dec 2020 00:00:00 +0000</pubDate><guid>https://chulheeyun.github.io/publication/yun2020n/</guid><description/></item><item><title>Low-Rank Bottleneck in Multi-head Attention Models</title><link>https://chulheeyun.github.io/publication/bhojanapalli2020low/</link><pubDate>Wed, 01 Jul 2020 00:00:00 +0000</pubDate><guid>https://chulheeyun.github.io/publication/bhojanapalli2020low/</guid><description/></item><item><title>Are Transformers universal approximators of sequence-to-sequence functions?</title><link>https://chulheeyun.github.io/publication/yun2020transformers/</link><pubDate>Fri, 01 May 2020 00:00:00 +0000</pubDate><guid>https://chulheeyun.github.io/publication/yun2020transformers/</guid><description/></item><item><title>Are deep ResNets provably better than linear predictors?</title><link>https://chulheeyun.github.io/publication/yun2019deep/</link><pubDate>Mon, 02 Dec 2019 00:00:00 +0000</pubDate><guid>https://chulheeyun.github.io/publication/yun2019deep/</guid><description/></item><item><title>Small ReLU networks are powerful memorizers: a tight analysis of memorization capacity</title><link>https://chulheeyun.github.io/publication/yun2019small2/</link><pubDate>Sun, 01 Dec 2019 00:00:00 +0000</pubDate><guid>https://chulheeyun.github.io/publication/yun2019small2/</guid><description/></item><item><title>Efficiently testing local optimality and escaping saddles for ReLU networks</title><link>https://chulheeyun.github.io/publication/yun2019efficiently/</link><pubDate>Thu, 02 May 2019 00:00:00 +0000</pubDate><guid>https://chulheeyun.github.io/publication/yun2019efficiently/</guid><description/></item><item><title>Small nonlinearities in activation functions create bad local minima in neural networks</title><link>https://chulheeyun.github.io/publication/yun2019small/</link><pubDate>Wed, 01 May 2019 00:00:00 +0000</pubDate><guid>https://chulheeyun.github.io/publication/yun2019small/</guid><description/></item><item><title>Minimax Bounds on Stochastic Batched Convex Optimization</title><link>https://chulheeyun.github.io/publication/duchi2018minimax/</link><pubDate>Sun, 01 Jul 2018 00:00:00 +0000</pubDate><guid>https://chulheeyun.github.io/publication/duchi2018minimax/</guid><description/></item><item><title>Global optimality conditions for deep neural networks</title><link>https://chulheeyun.github.io/publication/yun2018global/</link><pubDate>Sun, 01 Apr 2018 00:00:00 +0000</pubDate><guid>https://chulheeyun.github.io/publication/yun2018global/</guid><description/></item><item><title>Face detection using Local Hybrid Patterns</title><link>https://chulheeyun.github.io/publication/yun2015face/</link><pubDate>Sun, 19 Apr 2015 00:00:00 +0000</pubDate><guid>https://chulheeyun.github.io/publication/yun2015face/</guid><description/></item></channel></rss>