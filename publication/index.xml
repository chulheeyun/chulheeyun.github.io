<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Publications | Chulhee “Charlie” Yun</title><link>https://chulheeyun.github.io/publication/</link><atom:link href="https://chulheeyun.github.io/publication/index.xml" rel="self" type="application/rss+xml"/><description>Publications</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Sun, 25 Jun 2023 00:00:00 +0000</lastBuildDate><image><url>https://chulheeyun.github.io/media/icon_huab7a42417b42d6935dd5e37419205d92_14830_512x512_fill_lanczos_center_3.png</url><title>Publications</title><link>https://chulheeyun.github.io/publication/</link></image><item><title>Enhancing Generalization and Plasticity for Sample Efficient Reinforcement Learning</title><link>https://chulheeyun.github.io/publication/lee2023enhancing/</link><pubDate>Sun, 25 Jun 2023 00:00:00 +0000</pubDate><guid>https://chulheeyun.github.io/publication/lee2023enhancing/</guid><description/></item><item><title>Practical Sharpness-Aware Minimization Cannot Converge All the Way to Optima</title><link>https://chulheeyun.github.io/publication/si2023practical/</link><pubDate>Thu, 15 Jun 2023 00:00:00 +0000</pubDate><guid>https://chulheeyun.github.io/publication/si2023practical/</guid><description/></item><item><title>Provable Benefit of Mixup for Finding Optimal Decision Boundaries</title><link>https://chulheeyun.github.io/publication/oh2023provable/</link><pubDate>Tue, 25 Apr 2023 00:00:00 +0000</pubDate><guid>https://chulheeyun.github.io/publication/oh2023provable/</guid><description/></item><item><title>Tighter Lower Bounds for Shuffling SGD: Random Permutations and Beyond</title><link>https://chulheeyun.github.io/publication/cha2023tighter/</link><pubDate>Tue, 14 Mar 2023 00:00:00 +0000</pubDate><guid>https://chulheeyun.github.io/publication/cha2023tighter/</guid><description/></item><item><title>On the Training Instability of Shuffling SGD with Batch Normalization</title><link>https://chulheeyun.github.io/publication/wu2023training/</link><pubDate>Fri, 24 Feb 2023 00:00:00 +0000</pubDate><guid>https://chulheeyun.github.io/publication/wu2023training/</guid><description/></item><item><title>SGDA with shuffling: faster convergence for nonconvex-PŁ minimax optimization</title><link>https://chulheeyun.github.io/publication/cho2023sgda/</link><pubDate>Wed, 12 Oct 2022 00:00:00 +0000</pubDate><guid>https://chulheeyun.github.io/publication/cho2023sgda/</guid><description/></item><item><title>Minibatch vs Local SGD with Shuffling: Tight Convergence Bounds and Beyond</title><link>https://chulheeyun.github.io/publication/yun2022minibatch/</link><pubDate>Fri, 01 Apr 2022 00:00:00 +0000</pubDate><guid>https://chulheeyun.github.io/publication/yun2022minibatch/</guid><description/></item><item><title>Open Problem: Can Single-Shuffle SGD be Better than Reshuffling SGD and GD?</title><link>https://chulheeyun.github.io/publication/yun2021can/</link><pubDate>Mon, 02 Aug 2021 00:00:00 +0000</pubDate><guid>https://chulheeyun.github.io/publication/yun2021can/</guid><description/></item><item><title>Provable Memorization via Deep Neural Networks using Sub-linear Parameters</title><link>https://chulheeyun.github.io/publication/park2021provable/</link><pubDate>Sun, 01 Aug 2021 00:00:00 +0000</pubDate><guid>https://chulheeyun.github.io/publication/park2021provable/</guid><description/></item><item><title>A Unifying View on Implicit Bias in Training Linear Neural Networks</title><link>https://chulheeyun.github.io/publication/yun2021unifying/</link><pubDate>Sun, 02 May 2021 00:00:00 +0000</pubDate><guid>https://chulheeyun.github.io/publication/yun2021unifying/</guid><description/></item><item><title>Minimum Width for Universal Approximation</title><link>https://chulheeyun.github.io/publication/park2021minimum/</link><pubDate>Sat, 01 May 2021 00:00:00 +0000</pubDate><guid>https://chulheeyun.github.io/publication/park2021minimum/</guid><description/></item><item><title>SGD with shuffling: optimal rates without component convexity and large epoch requirements</title><link>https://chulheeyun.github.io/publication/ahn2020sgd/</link><pubDate>Wed, 02 Dec 2020 00:00:00 +0000</pubDate><guid>https://chulheeyun.github.io/publication/ahn2020sgd/</guid><description/></item><item><title>$O(n)$ Connections are Expressive Enough: Universal Approximability of Sparse Transformers</title><link>https://chulheeyun.github.io/publication/yun2020n/</link><pubDate>Tue, 01 Dec 2020 00:00:00 +0000</pubDate><guid>https://chulheeyun.github.io/publication/yun2020n/</guid><description/></item><item><title>Low-Rank Bottleneck in Multi-head Attention Models</title><link>https://chulheeyun.github.io/publication/bhojanapalli2020low/</link><pubDate>Wed, 01 Jul 2020 00:00:00 +0000</pubDate><guid>https://chulheeyun.github.io/publication/bhojanapalli2020low/</guid><description/></item><item><title>Are Transformers universal approximators of sequence-to-sequence functions?</title><link>https://chulheeyun.github.io/publication/yun2020transformers/</link><pubDate>Fri, 01 May 2020 00:00:00 +0000</pubDate><guid>https://chulheeyun.github.io/publication/yun2020transformers/</guid><description/></item><item><title>Are deep ResNets provably better than linear predictors?</title><link>https://chulheeyun.github.io/publication/yun2019deep/</link><pubDate>Mon, 02 Dec 2019 00:00:00 +0000</pubDate><guid>https://chulheeyun.github.io/publication/yun2019deep/</guid><description/></item><item><title>Small ReLU networks are powerful memorizers: a tight analysis of memorization capacity</title><link>https://chulheeyun.github.io/publication/yun2019small2/</link><pubDate>Sun, 01 Dec 2019 00:00:00 +0000</pubDate><guid>https://chulheeyun.github.io/publication/yun2019small2/</guid><description/></item><item><title>Efficiently testing local optimality and escaping saddles for ReLU networks</title><link>https://chulheeyun.github.io/publication/yun2019efficiently/</link><pubDate>Thu, 02 May 2019 00:00:00 +0000</pubDate><guid>https://chulheeyun.github.io/publication/yun2019efficiently/</guid><description/></item><item><title>Small nonlinearities in activation functions create bad local minima in neural networks</title><link>https://chulheeyun.github.io/publication/yun2019small/</link><pubDate>Wed, 01 May 2019 00:00:00 +0000</pubDate><guid>https://chulheeyun.github.io/publication/yun2019small/</guid><description/></item><item><title>Minimax Bounds on Stochastic Batched Convex Optimization</title><link>https://chulheeyun.github.io/publication/duchi2018minimax/</link><pubDate>Sun, 01 Jul 2018 00:00:00 +0000</pubDate><guid>https://chulheeyun.github.io/publication/duchi2018minimax/</guid><description/></item><item><title>Global optimality conditions for deep neural networks</title><link>https://chulheeyun.github.io/publication/yun2018global/</link><pubDate>Sun, 01 Apr 2018 00:00:00 +0000</pubDate><guid>https://chulheeyun.github.io/publication/yun2018global/</guid><description/></item><item><title>Face detection using Local Hybrid Patterns</title><link>https://chulheeyun.github.io/publication/yun2015face/</link><pubDate>Sun, 19 Apr 2015 00:00:00 +0000</pubDate><guid>https://chulheeyun.github.io/publication/yun2015face/</guid><description/></item></channel></rss>