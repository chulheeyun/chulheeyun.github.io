<!doctype html><html lang=en-us>
<head>
<meta charset=utf-8>
<meta name=viewport content="width=device-width,initial-scale=1">
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=generator content="Wowchemy 5.4.0 for Hugo">
<link rel=preconnect href=https://fonts.gstatic.com crossorigin>
<link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap">
<link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload="this.media='all'">
<meta name=author content="Chulhee Yun">
<meta name=description content="Assistant Professor">
<link rel=alternate hreflang=en-us href=https://chulheeyun.github.io/>
<meta name=theme-color content="#bbdefb">
<script src=/js/mathjax-config.js></script>
<link rel=stylesheet href=/css/vendor-bundle.min.f1ecf783c14edc00c9320c205831ad8e.css media=print onload="this.media='all'">
<link rel=stylesheet href=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/styles/github.min.css crossorigin=anonymous title=hl-light media=print onload="this.media='all'" disabled>
<link rel=stylesheet href=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/styles/dracula.min.css crossorigin=anonymous title=hl-dark media=print onload="this.media='all'">
<script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js integrity crossorigin=anonymous async></script>
<link rel=stylesheet href=/css/wowchemy.94ce078b6ba7df2440fe0463dc38f20e.css>
<script src=https://identity.netlify.com/v1/netlify-identity-widget.js></script>
<link rel=alternate href=/index.xml type=application/rss+xml title="Chulhee “Charlie” Yun">
<link rel=manifest href=/manifest.webmanifest>
<link rel=icon type=image/png href=/media/icon_huab7a42417b42d6935dd5e37419205d92_14830_32x32_fill_lanczos_center_3.png>
<link rel=apple-touch-icon type=image/png href=/media/icon_huab7a42417b42d6935dd5e37419205d92_14830_180x180_fill_lanczos_center_3.png>
<link rel=canonical href=https://chulheeyun.github.io/>
<meta property="twitter:card" content="summary">
<meta property="og:site_name" content="Chulhee “Charlie” Yun">
<meta property="og:url" content="https://chulheeyun.github.io/">
<meta property="og:title" content="Chulhee “Charlie” Yun">
<meta property="og:description" content="Assistant Professor"><meta property="og:image" content="https://chulheeyun.github.io/media/icon_huab7a42417b42d6935dd5e37419205d92_14830_512x512_fill_lanczos_center_3.png">
<meta property="twitter:image" content="https://chulheeyun.github.io/media/icon_huab7a42417b42d6935dd5e37419205d92_14830_512x512_fill_lanczos_center_3.png"><meta property="og:locale" content="en-us">
<meta property="og:updated_time" content="2030-06-01T13:00:00+00:00">
<script type=application/ld+json>{"@context":"https://schema.org","@type":"WebSite","potentialAction":{"@type":"SearchAction","target":"https://chulheeyun.github.io/?q={search_term_string}","query-input":"required name=search_term_string"},"url":"https://chulheeyun.github.io/"}</script>
<title>Chulhee “Charlie” Yun</title>
</head>
<body id=top data-spy=scroll data-offset=70 data-target=#navbar-main class="page-wrapper dark">
<script src=/js/wowchemy-init.min.d333d96014f7bcfa311294a92ace0d60.js></script>
<aside class=search-modal id=search>
<div class=container>
<section class=search-header>
<div class="row no-gutters justify-content-between mb-3">
<div class=col-6>
<h1>Search</h1>
</div>
<div class="col-6 col-search-close">
<a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a>
</div>
</div>
<div id=search-box>
<input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...>
</div>
</section>
<section class=section-search-results>
<div id=search-hits>
</div>
</section>
</div>
</aside>
<div class=page-header>
<header class=header--fixed>
<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main>
<div class=container-xl>
<div class="d-none d-lg-inline-flex">
<a class=navbar-brand href=/>Chulhee “Charlie” Yun</a>
</div>
<button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span>
</button>
<div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
<a class=navbar-brand href=/>Chulhee “Charlie” Yun</a>
</div>
<div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content>
<ul class="navbar-nav d-md-inline-flex">
<li class=nav-item>
<a class=nav-link href=/#about data-target=#about><span>Home</span></a>
</li>
<li class=nav-item>
<a class=nav-link href=/#news data-target=#news><span>News</span></a>
</li>
<li class=nav-item>
<a class=nav-link href=/#publications data-target=#publications><span>Publications</span></a>
</li>
<li class=nav-item>
<a class=nav-link href=/#teaching data-target=#teaching><span>Teaching</span></a>
</li>
<li class=nav-item>
<a class=nav-link href=/#group data-target=#group><span>Research Group</span></a>
</li>
<li class=nav-item>
<a class=nav-link href=/#service data-target=#service><span>Service</span></a>
</li>
</ul>
</div>
<ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">
<li class=nav-item>
<a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a>
</li>
</ul>
</div>
</nav>
</header>
</div>
<div class=page-body>
<span class="js-widget-page d-none"></span>
<section id=about class="home-section wg-about">
<div class=home-section-bg>
</div>
<div class=container>
<div class=row>
<div class="col-12 col-lg-4">
<div id=profile>
<img class="avatar avatar-circle" width=270 height=270 src=/authors/chulhee-yun/avatar_hu25d333fcded103db2f52f7476cb1420a_326069_270x270_fill_q75_lanczos_center.jpg alt="Chulhee Yun">
<div class=portrait-title>
<h2>Chulhee Yun</h2>
<h3>Assistant Professor</h3>
<h3>
<a href=https://gsai.kaist.ac.kr/ target=_blank rel=noopener>
<span>Kim Jaechul Graduate School of AI</span>
</a>
</h3>
<h3>
<a href=https://www.kaist.ac.kr/ target=_blank rel=noopener>
<span>KAIST</span>
</a>
</h3>
</div>
<ul class=network-icon aria-hidden=true>
<li>
<a href="https://scholar.google.com/citations?user=Ukl64ggAAAAJ" target=_blank rel=noopener aria-label=graduation-cap>
<i class="fas fa-graduation-cap big-icon"></i>
</a>
</li>
</ul>
</div>
</div>
<div class="col-12 col-lg-8">
<div class=article-style>
<p>My name is Chulhee (I go by <strong>Charlie</strong>), and I am an assistant professor at <a href=https://gsai.kaist.ac.kr/ target=_blank rel=noopener>KAIST Kim Jaechul Graduate School of AI (KAIST AI)</a>.
I direct the Optimization & Machine Learning (OptiML) Laboratory at KAIST AI.</p>
<p>I finished my PhD from the <a href=https://lids.mit.edu/ target=_blank rel=noopener>Laboratory for Information and Decision Systems</a> at <a href=https://web.mit.edu/ target=_blank rel=noopener>Massachusetts Institute of Technology</a>, where I was fortunate to study under the joint supervision of <a href=http://optml.mit.edu/ target=_blank rel=noopener>Prof. Suvrit Sra</a> and <a href=https://jadbabaie.mit.edu/ target=_blank rel=noopener>Prof. Ali Jadbabaie</a>.
Before MIT, I was a master’s student in <a href=https://ee.stanford.edu/ target=_blank rel=noopener>Electrical Engineering</a> at <a href=https://www.stanford.edu/ target=_blank rel=noopener>Stanford University</a>, where I worked with <a href=https://stanford.edu/~jduchi/ target=_blank rel=noopener>Prof. John Duchi</a>.
I finished my undergraduate program in <a href=https://ee.kaist.ac.kr/ target=_blank rel=noopener>Electrical Engineering</a> at <a href=https://www.kaist.ac.kr/ target=_blank rel=noopener>KAIST</a>.</p>
<p><strong>Email</strong>: {firstname}.{lastname}@kaist.ac.kr<br>
<strong>Phone</strong>: +82-2-958-3919<br>
<strong>Office</strong>: KAIST Seoul Campus Building #9, 9401</p>
</br>
<p><strong>For prospective students</strong>: If you are curious about what kinds of research I do, please see this <a href="https://www.eiric.or.kr/manpower/rising_view.php?Seq=43" target=_blank rel=noopener>interview</a> article (in Korean). I look for <strong>self-motivated</strong> graduate students with <strong>strong mathematical backgrounds</strong>. If you are an undergraduate student interested in interning at our lab, consider applying for summer/winter KAIST AI Research Internship (<strong>KAIRI</strong>) programs.</p>
</br>
</div>
<div class=row>
<div class=col-md-5>
<div class=section-subheading>Interests</div>
<ul class="ul-interests mb-0">
<li>Deep Learning Theory</li>
<li>Optimization</li>
<li>Machine Learning Theory</li>
</ul>
</div>
<div class=col-md-7>
<div class=section-subheading>Education</div>
<ul class="ul-edu fa-ul mb-0">
<li>
<i class="fa-li fas fa-graduation-cap"></i>
<div class=description>
<p class=course>PhD in Elec. Eng. & Comp. Sci., 2016–2021</p>
<p class=institution>Massachusetts Institute of Technology</p>
</div>
</li>
<li>
<i class="fa-li fas fa-graduation-cap"></i>
<div class=description>
<p class=course>MSc in Electrical Engineering, 2014–2016</p>
<p class=institution>Stanford University</p>
</div>
</li>
<li>
<i class="fa-li fas fa-graduation-cap"></i>
<div class=description>
<p class=course>BSc in Electrical Engineering, 2007–2014</p>
<p class=institution>KAIST</p>
</div>
</li>
</ul>
</div>
</div>
</div>
</div>
</div>
</section>
<section id=news class="home-section wg-pages">
<div class=home-section-bg>
</div>
<div class=container>
<div class="row justify-content-center">
<div class="section-heading col-12 mb-3 text-center">
<h1 class=mb-0>News</h1>
</div>
<div class=col-12>
<div class="view-list view-list-item">
<i class="fas fa-newspaper pub-icon" aria-hidden=true></i>
[Jun 2024] One paper got accepted to the ICML 2024 Workshop on <a href=https://longcontextfm.github.io/ target=_blank>Long-Context Foundation Models</a> and another paper to appear at the ICML 2024 Workshop on <a href=https://want-ai-hpc.github.io/icml2024/ target=_blank>Advancing Neural Network Training: Computational Efficiency, Scalability, and Resource Optimization</a>.
</div>
<div class="view-list view-list-item">
<i class="fas fa-newspaper pub-icon" aria-hidden=true></i>
[Jun 2024] Three papers got accepted to the ICML 2024 Workshop on <a href=https://sites.google.com/corp/view/hidimlearning/home target=_blank>High-dimensional Learning Dynamics 2024: The Emergence of Structure and Reasoning</a>.
</div>
<div class="view-list view-list-item">
<i class="fas fa-newspaper pub-icon" aria-hidden=true></i>
[May 2024] Our <a href=https://arxiv.org/abs/2402.10475 target=_blank>paper</a> on the benefit of alternating updates in minimax optimization got accepted to ICML 2024! (update: the paper was selected as a <strong>spotlight paper!</strong>)
</div>
<div class="view-list view-list-item">
<i class="fas fa-newspaper pub-icon" aria-hidden=true></i>
[Mar 2024] One <a href=https://arxiv.org/abs/2402.10475 target=_blank>paper</a> was accepted to the ICLR 2024 Workshop on <a href=https://sites.google.com/view/bgpt-iclr24 target=_blank>Bridging the Gap Between Practice and Theory in Deep Learning</a>. We prove that alternating gradient descent-ascent (GDA) converges faster than simultaneous GDA, and propose an even faster variant!
</div>
<div class="view-list view-list-item">
<i class="fas fa-newspaper pub-icon" aria-hidden=true></i>
[Jan 2024] Our <a href=https://arxiv.org/abs/2310.01082 target=_blank>paper</a> on the optimization characteristics of linear Transformers got accepted to ICLR 2024!
</div>
<div class=see-all>
<a href=/news/>
See all
<i class="fas fa-angle-right"></i>
</a>
</div>
</div>
</div>
</div>
</section>
<section id=publications class="home-section wg-pages">
<div class=home-section-bg>
</div>
<div class=container>
<div class="row justify-content-center">
<div class="section-heading col-12 mb-3 text-center">
<h1 class=mb-0>Publications</h1>
</div>
<div class=col-12>
<div class="pub-list-item view-citation" style=margin-bottom:1rem>
<i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<a href=/publication/shin2024dash/>DASH: Warm-Starting Neural Network Training Without Loss of Plasticity Under Stationarity</a>&nbsp;
</br>
<span class="article-metadata li-cite-author">
<span>
Baekrok Shin</span>, <span>
Junsoo Oh</span>, <span>
Hanseul Cho</span>, <span class=author-highlighted>
Chulhee Yun</span></br>
</span>
<em>ICML 2024 Workshop on Advancing Neural Network Training: Computational Efficiency, Scalability, and Resource Optimization</em>
</br>
</div>
<div class="pub-list-item view-citation" style=margin-bottom:1rem>
<i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<a href=/publication/oh2024provable/>Provable Benefit of Cutout and CutMix for Feature Learning</a>&nbsp;
</br>
<span class="article-metadata li-cite-author">
<span>
Junsoo Oh</span>, <span class=author-highlighted>
Chulhee Yun</span></br>
</span>
<em>ICML 2024 Workshop on High-dimensional Learning Dynamics 2024: The Emergence of Structure and Reasoning</em>
</br>
</div>
<div class="pub-list-item view-citation" style=margin-bottom:1rem>
<i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<a href=/publication/cho2024position/>Position Coupling: Leveraging Task Structure for Improved Length Generalization of Transformers</a>&nbsp;
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2405.20671 target=_blank rel=noopener>
arXiv</a>
</br>
<span class="article-metadata li-cite-author">
<span>
Hanseul Cho</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>
Jaeyoung Cha</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>
Pranjal Awasthi</span>, <span>
Srinadh Bhojanapalli</span>, <span>
Anupam Gupta</span>, <span class=author-highlighted>
Chulhee Yun</span></br>
</span>
<em>ICML 2024 Workshop on Long-Context Foundation Models</em>
</br>
</div>
<div class="pub-list-item view-citation" style=margin-bottom:1rem>
<i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<a href=/publication/song2024does/>Does SGD really happen in tiny subspaces?</a>&nbsp;
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2405.16002 target=_blank rel=noopener>
arXiv</a>
</br>
<span class="article-metadata li-cite-author">
<span>
Minhak Song</span>, <span>
Kwangjun Ahn</span>, <span class=author-highlighted>
Chulhee Yun</span></br>
</span>
<em>ICML 2024 Workshop on High-dimensional Learning Dynamics 2024: The Emergence of Structure and Reasoning</em>
</br>
</div>
<div class="pub-list-item view-citation" style=margin-bottom:1rem>
<i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<a href=/publication/phunyaphibarn2023large/>Gradient Descent with Polyak's Momentum Finds Flatter Minima via Large Catapults</a>&nbsp;
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2311.15051 target=_blank rel=noopener>
arXiv</a>
</br>
<span class="article-metadata li-cite-author">
<span>
Prin Phunyaphibarn</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>
Junghyun Lee</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>
Bohan Wang</span>, <span>
Huishuai Zhang</span>, <span class=author-highlighted>
Chulhee Yun</span></br>
</span>
<em>ICML 2024 Workshop on High-dimensional Learning Dynamics 2024: The Emergence of Structure and Reasoning</em>
</br>
<em>NeurIPS 2023 Workshop on Mathematics of Modern Machine Learning</em> <em><strong>(Oral)</strong></em></br>
</div>
<div class="pub-list-item view-citation" style=margin-bottom:1rem>
<i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<a href=/publication/lee2024fundamental/>Fundamental Benefit of Alternating Updates in Minimax Optimization</a>&nbsp;
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2402.10475 target=_blank rel=noopener>
arXiv</a>
</br>
<span class="article-metadata li-cite-author">
<span>
Jaewook Lee</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>
Hanseul Cho</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span class=author-highlighted>
Chulhee Yun</span></br>
</span>
<em>International Conference on Machine Learning (ICML) 2024</em> <em><strong>(Spotlight)</strong></em>
</br>
<em>ICLR 2024 Workshop on Bridging the Gap Between Practice and Theory in Deep Learning</em></br>
</div>
<div class="pub-list-item view-citation" style=margin-bottom:1rem>
<i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<a href=/publication/ahn2023linear/>Linear attention is (maybe) all you need (to understand transformer optimization)</a>&nbsp;
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://openreview.net/forum?id=0uI5415ry7" target=_blank rel=noopener>
Paper</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2310.01082 target=_blank rel=noopener>
arXiv</a>
</br>
<span class="article-metadata li-cite-author">
<span>
Kwangjun Ahn</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>
Xiang Cheng</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>
Minhak Song</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span class=author-highlighted>
Chulhee Yun</span>, <span>
Suvrit Sra</span>, <span>
Ali Jadbabaie</span></br>
</span>
<em>International Conference on Learning Representations (ICLR) 2024</em>
</br>
<em>NeurIPS 2023 Workshop on Mathematics of Modern Machine Learning</em> <em><strong>(Oral)</strong></em></br>
</div>
<div class="pub-list-item view-citation" style=margin-bottom:1rem>
<i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<a href=/publication/lee2023fair/>Fair Streaming Principal Component Analysis: Statistical and Algorithmic Viewpoint</a>&nbsp;
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://papers.nips.cc/paper_files/paper/2023/hash/1074541383db5ef12d6ac66d2f8e8d34-Abstract-Conference.html target=_blank rel=noopener>
Paper</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2310.18593 target=_blank rel=noopener>
arXiv</a>
</br>
<span class="article-metadata li-cite-author">
<span>
Junghyun Lee</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>
Hanseul Cho</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>
Se-Young Yun</span>, <span class=author-highlighted>
Chulhee Yun</span></br>
</span>
<em>Neural Information Processing Systems (NeurIPS) 2023</em>
</br>
</div>
<div class="pub-list-item view-citation" style=margin-bottom:1rem>
<i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<a href=/publication/song2023trajectory/>Trajectory Alignment: Understanding the Edge of Stability Phenomenon via Bifurcation Theory</a>&nbsp;
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://papers.nips.cc/paper_files/paper/2023/hash/e2a9256bd816ab9e082dfaa22f1f62a2-Abstract-Conference.html target=_blank rel=noopener>
Paper</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2307.04204 target=_blank rel=noopener>
arXiv</a>
</br>
<span class="article-metadata li-cite-author">
<span>
Minhak Song</span>, <span class=author-highlighted>
Chulhee Yun</span></br>
</span>
<em>Neural Information Processing Systems (NeurIPS) 2023</em>
</br>
</div>
<div class="pub-list-item view-citation" style=margin-bottom:1rem>
<i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<a href=/publication/lee2023enhancing/>PLASTIC: Improving Input and Label Plasticity for Sample Efficient Reinforcement Learning</a>&nbsp;
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://papers.nips.cc/paper_files/paper/2023/hash/c464fc4516aca4e68f2a14e67c6f0402-Abstract-Conference.html target=_blank rel=noopener>
Paper</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2306.10711 target=_blank rel=noopener>
arXiv</a>
</br>
<span class="article-metadata li-cite-author">
<span>
Hojoon Lee</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>
Hanseul Cho</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>
Hyunseung Kim</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>
Daehoon Gwak</span>, <span>
Joonkee Kim</span>, <span>
Jaegul Choo</span>, <span>
Se-Young Yun</span>, <span class=author-highlighted>
Chulhee Yun</span></br>
</span>
<em>Neural Information Processing Systems (NeurIPS) 2023</em>
</br>
</div>
<div class="pub-list-item view-citation" style=margin-bottom:1rem>
<i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<a href=/publication/si2023practical/>Practical Sharpness-Aware Minimization Cannot Converge All the Way to Optima</a>&nbsp;
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://papers.nips.cc/paper_files/paper/2023/hash/5305b7891e1098dd9773d35cd9333180-Abstract-Conference.html target=_blank rel=noopener>
Paper</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2306.09850 target=_blank rel=noopener>
arXiv</a>
</br>
<span class="article-metadata li-cite-author">
<span>
Dongkuk Si</span>, <span class=author-highlighted>
Chulhee Yun</span></br>
</span>
<em>Neural Information Processing Systems (NeurIPS) 2023</em> <em><strong>(Spotlight)</strong></em>
</br>
<strong>Outstanding Paper Award</strong> at KAIA Conference 2023 (CKAIA 2023)</br>
</div>
<div class="pub-list-item view-citation" style=margin-bottom:1rem>
<i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<a href=/publication/oh2023provable/>Provable Benefit of Mixup for Finding Optimal Decision Boundaries</a>&nbsp;
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://proceedings.mlr.press/v202/oh23a.html target=_blank rel=noopener>
Paper</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2306.00267 target=_blank rel=noopener>
arXiv</a>
</br>
<span class="article-metadata li-cite-author">
<span>
Junsoo Oh</span>, <span class=author-highlighted>
Chulhee Yun</span></br>
</span>
<em>International Conference on Machine Learning (ICML) 2023</em>
</br>
</div>
<div class="pub-list-item view-citation" style=margin-bottom:1rem>
<i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<a href=/publication/cha2023tighter/>Tighter Lower Bounds for Shuffling SGD: Random Permutations and Beyond</a>&nbsp;
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://proceedings.mlr.press/v202/cha23a.html target=_blank rel=noopener>
Paper</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2303.07160 target=_blank rel=noopener>
arXiv</a>
</br>
<span class="article-metadata li-cite-author">
<span>
Jaeyoung Cha</span>, <span>
Jaewook Lee</span>, <span class=author-highlighted>
Chulhee Yun</span></br>
</span>
<em>International Conference on Machine Learning (ICML) 2023</em> <em><strong>(Oral)</strong></em>
</br>
</div>
<div class="pub-list-item view-citation" style=margin-bottom:1rem>
<i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<a href=/publication/wu2023training/>On the Training Instability of Shuffling SGD with Batch Normalization</a>&nbsp;
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://proceedings.mlr.press/v202/wu23x.html target=_blank rel=noopener>
Paper</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2302.12444 target=_blank rel=noopener>
arXiv</a>
</br>
<span class="article-metadata li-cite-author">
<span>
David X. Wu</span>, <span class=author-highlighted>
Chulhee Yun</span>, <span>
Suvrit Sra</span></br>
</span>
<em>International Conference on Machine Learning (ICML) 2023</em>
</br>
</div>
<div class="pub-list-item view-citation" style=margin-bottom:1rem>
<i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<a href=/publication/cho2023sgda/>SGDA with shuffling: faster convergence for nonconvex-PŁ minimax optimization</a>&nbsp;
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://openreview.net/forum?id=6xXtM8bFFJ" target=_blank rel=noopener>
Paper</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2210.05995 target=_blank rel=noopener>
arXiv</a>
</br>
<span class="article-metadata li-cite-author">
<span>
Hanseul Cho</span>, <span class=author-highlighted>
Chulhee Yun</span></br>
</span>
<em>International Conference on Learning Representations (ICLR) 2023</em>
</br>
<strong>NAVER Outstanding Theory Paper Award</strong> at KAIA-NAVER Joint Conference 2022 (JKAIA 2022)</br>
</div>
<div class="pub-list-item view-citation" style=margin-bottom:1rem>
<i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<a href=/publication/yun2022minibatch/>Minibatch vs Local SGD with Shuffling: Tight Convergence Bounds and Beyond</a>&nbsp;
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://openreview.net/forum?id=LdlwbBP2mlq" target=_blank rel=noopener>
Paper</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2110.10342 target=_blank rel=noopener>
arXiv</a>
</br>
<span class="article-metadata li-cite-author">
<span class=author-highlighted>
Chulhee Yun</span>, <span>
Shashank Rajput</span>, <span>
Suvrit Sra</span></br>
</span>
<em>International Conference on Learning Representations (ICLR) 2022</em> <em><strong>(Oral)</strong></em>
</br>
</div>
<div class="pub-list-item view-citation" style=margin-bottom:1rem>
<i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<a href=/publication/yun2021can/>Open Problem: Can Single-Shuffle SGD be Better than Reshuffling SGD and GD?</a>&nbsp;
<a class="btn btn-outline-primary btn-page-header btn-sm" href=http://proceedings.mlr.press/v134/open-problem-yun21a.html target=_blank rel=noopener>
Paper</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2103.07079 target=_blank rel=noopener>
Long version</a>
</br>
<span class="article-metadata li-cite-author">
<span class=author-highlighted>
Chulhee Yun</span>, <span>
Suvrit Sra</span>, <span>
Ali Jadbabaie</span></br>
</span>
<em>Conference on Learning Theory (COLT) 2021</em>
</br>
</div>
<div class="pub-list-item view-citation" style=margin-bottom:1rem>
<i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<a href=/publication/park2021provable/>Provable Memorization via Deep Neural Networks using Sub-linear Parameters</a>&nbsp;
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://proceedings.mlr.press/v134/park21a.html target=_blank rel=noopener>
Paper</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2010.13363 target=_blank rel=noopener>
arXiv</a>
</br>
<span class="article-metadata li-cite-author">
<span>
Sejun Park</span>, <span>
Jaeho Lee</span>, <span class=author-highlighted>
Chulhee Yun</span>, <span>
Jinwoo Shin</span></br>
</span>
<em>Conference on Learning Theory (COLT) 2021</em>
</br>
Presented as part of a contributed talk at <em>DeepMath 2020</em></br>
</div>
<div class="pub-list-item view-citation" style=margin-bottom:1rem>
<i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<a href=/publication/yun2021unifying/>A Unifying View on Implicit Bias in Training Linear Neural Networks</a>&nbsp;
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://openreview.net/forum?id=ZsZM-4iMQkH" target=_blank rel=noopener>
Paper</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2010.02501 target=_blank rel=noopener>
arXiv</a>
</br>
<span class="article-metadata li-cite-author">
<span class=author-highlighted>
Chulhee Yun</span>, <span>
Shankar Krishnan</span>, <span>
Hossein Mobahi</span></br>
</span>
<em>International Conference on Learning Representations (ICLR) 2021</em>
</br>
<em>NeurIPS 2020 Workshop on Optimization for Machine Learning (OPT 2020)</em></br>
</div>
<div class="pub-list-item view-citation" style=margin-bottom:1rem>
<i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<a href=/publication/park2021minimum/>Minimum Width for Universal Approximation</a>&nbsp;
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://openreview.net/forum?id=O-XJwyoIF-k" target=_blank rel=noopener>
Paper</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2006.08859 target=_blank rel=noopener>
arXiv</a>
</br>
<span class="article-metadata li-cite-author">
<span>
Sejun Park</span>, <span class=author-highlighted>
Chulhee Yun</span>, <span>
Jaeho Lee</span>, <span>
Jinwoo Shin</span></br>
</span>
<em>International Conference on Learning Representations (ICLR) 2021</em> <em><strong>(Spotlight)</strong></em>
</br>
Presented as part of a contributed talk at <em>DeepMath 2020</em></br>
</div>
<div class="pub-list-item view-citation" style=margin-bottom:1rem>
<i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<a href=/publication/ahn2020sgd/>SGD with shuffling: optimal rates without component convexity and large epoch requirements</a>&nbsp;
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://papers.nips.cc/paper/2020/hash/cb8acb1dc9821bf74e6ca9068032d623-Abstract.html target=_blank rel=noopener>
Paper</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2006.06946 target=_blank rel=noopener>
arXiv</a>
</br>
<span class="article-metadata li-cite-author">
<span>
Kwangjun Ahn</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span class=author-highlighted>
Chulhee Yun</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>
Suvrit Sra</span></br>
</span>
<em>Neural Information Processing Systems (NeurIPS) 2020</em> <em><strong>(Spotlight)</strong></em>
</br>
</div>
<div class="pub-list-item view-citation" style=margin-bottom:1rem>
<i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<a href=/publication/yun2020n/>$O(n)$ Connections are Expressive Enough: Universal Approximability of Sparse Transformers</a>&nbsp;
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://proceedings.neurips.cc/paper/2020/hash/9ed27554c893b5bad850a422c3538c15-Abstract.html target=_blank rel=noopener>
Paper</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2006.04862 target=_blank rel=noopener>
arXiv</a>
</br>
<span class="article-metadata li-cite-author">
<span class=author-highlighted>
Chulhee Yun</span>, <span>
Yin-Wen Chang</span>, <span>
Srinadh Bhojanapalli</span>, <span>
Ankit Singh Rawat</span>, <span>
Sashank J. Reddi</span>, <span>
Sanjiv Kumar</span></br>
</span>
<em>Neural Information Processing Systems (NeurIPS) 2020</em>
</br>
</div>
<div class="pub-list-item view-citation" style=margin-bottom:1rem>
<i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<a href=/publication/bhojanapalli2020low/>Low-Rank Bottleneck in Multi-head Attention Models</a>&nbsp;
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://proceedings.mlr.press/v119/bhojanapalli20a.html target=_blank rel=noopener>
Paper</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2002.07028 target=_blank rel=noopener>
arXiv</a>
</br>
<span class="article-metadata li-cite-author">
<span>
Srinadh Bhojanapalli</span>, <span class=author-highlighted>
Chulhee Yun</span>, <span>
Ankit Singh Rawat</span>, <span>
Sashank J. Reddi</span>, <span>
Sanjiv Kumar</span></br>
</span>
<em>International Conference on Machine Learning (ICML) 2020</em>
</br>
</div>
<div class="pub-list-item view-citation" style=margin-bottom:1rem>
<i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<a href=/publication/yun2020transformers/>Are Transformers universal approximators of sequence-to-sequence functions?</a>&nbsp;
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://openreview.net/forum?id=ByxRM0Ntvr" target=_blank rel=noopener>
Paper</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/1912.10077 target=_blank rel=noopener>
arXiv</a>
</br>
<span class="article-metadata li-cite-author">
<span class=author-highlighted>
Chulhee Yun</span>, <span>
Srinadh Bhojanapalli</span>, <span>
Ankit Singh Rawat</span>, <span>
Sashank J. Reddi</span>, <span>
Sanjiv Kumar</span></br>
</span>
<em>International Conference on Learning Representations (ICLR) 2020</em>
</br>
<em>NeurIPS 2019 Workshop on Machine Learning with Guarantees</em></br>
<strong>Honorable Mention</strong> at NYAS Machine Learning Symposium 2020 Poster Awards</br>
</div>
<div class="pub-list-item view-citation" style=margin-bottom:1rem>
<i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<a href=/publication/yun2019deep/>Are deep ResNets provably better than linear predictors?</a>&nbsp;
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://proceedings.neurips.cc/paper/2019/hash/661c1c090ff5831a647202397c61d73c-Abstract.html target=_blank rel=noopener>
Paper</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/1907.03922 target=_blank rel=noopener>
arXiv</a>
</br>
<span class="article-metadata li-cite-author">
<span class=author-highlighted>
Chulhee Yun</span>, <span>
Suvrit Sra</span>, <span>
Ali Jadbabaie</span></br>
</span>
<em>Neural Information Processing Systems (NeurIPS) 2019</em>
</br>
</div>
<div class="pub-list-item view-citation" style=margin-bottom:1rem>
<i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<a href=/publication/yun2019small2/>Small ReLU networks are powerful memorizers: a tight analysis of memorization capacity</a>&nbsp;
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://papers.nips.cc/paper_files/paper/2019/hash/dbea3d0e2a17c170c412c74273778159-Abstract.html target=_blank rel=noopener>
Paper</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/1810.07770 target=_blank rel=noopener>
arXiv</a>
</br>
<span class="article-metadata li-cite-author">
<span class=author-highlighted>
Chulhee Yun</span>, <span>
Suvrit Sra</span>, <span>
Ali Jadbabaie</span></br>
</span>
<em>Neural Information Processing Systems (NeurIPS) 2019</em> <em><strong>(Spotlight)</strong></em>
</br>
</div>
<div class="pub-list-item view-citation" style=margin-bottom:1rem>
<i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<a href=/publication/yun2019efficiently/>Efficiently testing local optimality and escaping saddles for ReLU networks</a>&nbsp;
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://openreview.net/forum?id=HylTXn0qYX" target=_blank rel=noopener>
Paper</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/1809.10858 target=_blank rel=noopener>
arXiv</a>
</br>
<span class="article-metadata li-cite-author">
<span class=author-highlighted>
Chulhee Yun</span>, <span>
Suvrit Sra</span>, <span>
Ali Jadbabaie</span></br>
</span>
<em>International Conference on Learning Representations (ICLR) 2019</em>
</br>
</div>
<div class="pub-list-item view-citation" style=margin-bottom:1rem>
<i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<a href=/publication/yun2019small/>Small nonlinearities in activation functions create bad local minima in neural networks</a>&nbsp;
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://openreview.net/forum?id=rke_YiRct7" target=_blank rel=noopener>
Paper</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/1802.03487 target=_blank rel=noopener>
arXiv</a>
</br>
<span class="article-metadata li-cite-author">
<span class=author-highlighted>
Chulhee Yun</span>, <span>
Suvrit Sra</span>, <span>
Ali Jadbabaie</span></br>
</span>
<em>International Conference on Learning Representations (ICLR) 2019</em>
</br>
</div>
<div class="pub-list-item view-citation" style=margin-bottom:1rem>
<i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<a href=/publication/duchi2018minimax/>Minimax Bounds on Stochastic Batched Convex Optimization</a>&nbsp;
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://proceedings.mlr.press/v75/duchi18a.html target=_blank rel=noopener>
Paper</a>
</br>
<span class="article-metadata li-cite-author">
<span>
John Duchi</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Alphabetical order"></i>, <span>
Feng Ruan</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Alphabetical order"></i>, <span class=author-highlighted>
Chulhee Yun</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Alphabetical order"></i></br>
</span>
<em>Conference on Learning Theory (COLT) 2018</em>
</br>
</div>
<div class="pub-list-item view-citation" style=margin-bottom:1rem>
<i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<a href=/publication/yun2018global/>Global optimality conditions for deep neural networks</a>&nbsp;
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://openreview.net/forum?id=BJk7Gf-CZ" target=_blank rel=noopener>
Paper</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/1707.02444 target=_blank rel=noopener>
arXiv</a>
</br>
<span class="article-metadata li-cite-author">
<span class=author-highlighted>
Chulhee Yun</span>, <span>
Suvrit Sra</span>, <span>
Ali Jadbabaie</span></br>
</span>
<em>International Conference on Learning Representations (ICLR) 2018</em>
</br>
<em>NIPS 2017 Workshop on Deep Learning: Bridging Theory and Practice</em></br>
</div>
<div class="pub-list-item view-citation" style=margin-bottom:1rem>
<i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<a href=/publication/yun2015face/>Face detection using Local Hybrid Patterns</a>&nbsp;
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://ieeexplore.ieee.org/abstract/document/7178214 target=_blank rel=noopener>
Paper</a>
</br>
<span class="article-metadata li-cite-author">
<span class=author-highlighted>
Chulhee Yun</span>, <span>
Donghoon Lee</span>, <span>
Chang D. Yoo</span></br>
</span>
<em>IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2015</em>
</br>
</div>
</div>
</div>
</div>
</section>
<section id=teaching class="home-section wg-pages">
<div class=home-section-bg>
</div>
<div class=container>
<div class=row>
<div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start">
<h1 class=mb-0>Teaching</h1>
</div>
<div class="col-12 col-lg-8">
<div class="view-list view-list-item">
<i class="fas fa-chalkboard pub-icon" aria-hidden=true></i>
AI709 Advanced Deep Learning Theory
(2024S)
</div>
<div class="view-list view-list-item">
<i class="fas fa-chalkboard pub-icon" aria-hidden=true></i>
AI616 Deep Learning Theory
(2022S/F, 2023S/F)
</div>
</div>
</div>
</div>
</section>
<section id=group class="home-section wg-pages">
<div class=home-section-bg>
</div>
<div class=container>
<div class=row>
<div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start">
<h1 class=mb-0>Research Group</h1>
</div>
<div class="col-12 col-lg-8">
<p>I direct the Optimization & Machine Learning (OptiML) Laboratory at KAIST. I ambitiously pronounce it as the “Optimal Lab”&mdash;although my students may disagree!</p>
<div class="view-list view-list-item">
<i class="fas fa-arrow-circle-right pub-icon" aria-hidden=true></i>
<strong>PhD Students</strong> (all students are in KAIST AI)<br>
<ul>
<li>Jaeyoung Cha</li>
<li><a href=https://hanseuljo.github.io/ target=_blank>Hanseul Cho</a></li>
<li><a href=https://nick-jhlee.github.io target=_blank>Junghyun Lee</a> (co-advised with <a href=https://fbsqkd.github.io/ target=_blank>Prof. Se-Young Yun</a>)</li>
<li><a href=https://junsoo424.github.io/ target=_blank>Junsoo Oh</a></li>
</ul>
</div>
<div class="view-list view-list-item">
<i class="fas fa-arrow-circle-right pub-icon" aria-hidden=true></i>
<strong>Master&rsquo;s Students</strong> (all students are in KAIST AI)<br>
<ul>
<li><a href=https://id8198.github.io/ target=_blank>Jaewook Lee</a></li>
<li>Baekrok Shin</li>
<li>Geonhui Yoo</li>
</ul>
</div>
<div class="view-list view-list-item">
<i class="fas fa-arrow-circle-right pub-icon" aria-hidden=true></i>
<strong>Undergraduate Students/KAIRI Interns</strong><br>
<ul>
<li>Hyunji Jung (POSTECH Math/CS)</li>
<li>ChangMin Kang (KAIST Math)</li>
<li>Donghwa Kim (KAIST Math)</li>
<li>Yujun Kim (KAIST Math/CS)</li>
<li>Chaewon Moon (SNU IE/Math)</li>
<li><a href=https://songminhak.github.io/ target=_blank>Minhak Song</a> (KAIST ISysE/Math)</li>
</ul>
</div>
<div class="view-list view-list-item">
<i class="fas fa-arrow-circle-right pub-icon" aria-hidden=true></i>
<strong>Former Graduate Students</strong><br>
<ul>
<li><a href=https://dongkuksi.notion.site/Dongkuk-Si-826580ca76a04aab8c2722a22ca00741 target=_blank>Dongkuk Si</a> (MSc 2024)</li>
</ul>
</div>
</div>
</div>
</div>
</section>
<section id=service class="home-section wg-pages">
<div class=home-section-bg>
</div>
<div class=container>
<div class=row>
<div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start">
<h1 class=mb-0>Service</h1>
</div>
<div class="col-12 col-lg-8">
<div class="view-list view-list-item">
<i class="fas fa-balance-scale pub-icon" aria-hidden=true></i>
<strong>Conference Area Chair</strong><br>
<ul>
<li>NeurIPS 2023&ndash;2024</li>
</ul>
</div>
<div class="view-list view-list-item">
<i class="fas fa-balance-scale pub-icon" aria-hidden=true></i>
<strong>Conference/Workshop Reviewer</strong><br>
<ul>
<li>ICLR 2019&ndash;2024</li>
<li>ICML 2019&ndash;2024</li>
<li>COLT 2020&ndash;2024</li>
<li>NeurIPS 2018&ndash;2020, 2022</li>
<li>AISTATS 2019</li>
<li>CDC 2018</li>
<li>ICLR 2024 Privacy Regulation and Protection in Machine Learning Workshop</li>
</ul>
</div>
<div class="view-list view-list-item">
<i class="fas fa-balance-scale pub-icon" aria-hidden=true></i>
<strong>Journal Reviewer</strong><br>
<ul>
<li>Journal of Machine Learning Research</li>
<li>SIAM Journal on Mathematics of Data Science</li>
<li>Annals of Statistics</li>
<li>IEEE Transactions on Neural Networks and Learning Systems</li>
<li>IEEE Transactions on Information Theory</li>
<li>Mathematical Programming</li>
</ul>
</div>
<div class="view-list view-list-item">
<i class="fas fa-balance-scale pub-icon" aria-hidden=true></i>
<strong>(Co-)Organizer</strong><br>
<ul>
<li>Mathematical Theory of AI Sessions at KAIA-NAVER Joint Conference 2022</li>
<li>The LIDS & Stats Tea Talk Series Fall 2019&ndash;Spring 2020</li>
<li>The 24th Annual LIDS Student Conference 2019</li>
</ul>
</div>
</div>
</div>
</div>
</section>
</div>
<div class=page-footer>
<div class=container>
<footer class=site-footer>
<p class=powered-by>
Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> — the free, <a href=https://github.com/wowchemy/wowchemy-hugo-themes target=_blank rel=noopener>open source</a> website builder that empowers creators.
</p>
</footer>
</div>
</div>
<script src=/js/vendor-bundle.min.b73dfaac3b6499dc997741748a7c3fe2.js></script>
<script src=https://cdn.jsdelivr.net/gh/desandro/imagesloaded@v4.1.4/imagesloaded.pkgd.min.js integrity="sha512-S5PZ9GxJZO16tT9r3WJp/Safn31eu8uWrzglMahDT4dsmgqWonRY9grk3j+3tfuPr9WJNsfooOR7Gi7HL5W2jw==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/metafizzy/isotope@v3.0.6/dist/isotope.pkgd.min.js integrity="sha512-Zq2BOxyhvnRFXu0+WE6ojpZLOU2jdnqbrM1hmVdGzyeCa1DgM3X5Q4A/Is9xA1IkbUeDd7755dNNI/PzSf2Pew==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/highlight.min.js integrity="sha512-Ypjm0o7jOxAd4hpdoppSEN0TQOC19UtPAqD+4s5AlXmUvbmmS/YMxYqAqarQYyxTnB6/rqip9qcxlNB/3U9Wdg==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/languages/r.min.js crossorigin=anonymous></script>
<script id=search-hit-fuse-template type=text/x-template>
        <div class="search-hit" id="summary-{{key}}">
          <div class="search-hit-content">
            <div class="search-hit-name">
              <a href="{{relpermalink}}">{{title}}</a>
              <div class="article-metadata search-hit-type">{{type}}</div>
              <p class="search-hit-description">{{snippet}}</p>
            </div>
          </div>
        </div>
      </script>
<script src=https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin=anonymous></script>
<script id=page-data type=application/json>{"use_headroom":false}</script>
<script src=/en/js/wowchemy.min.f41ee09ba84b78e6bdd68fadc655c33f.js></script>
<div id=modal class="modal fade" role=dialog>
<div class=modal-dialog>
<div class=modal-content>
<div class=modal-header>
<h5 class=modal-title>Cite</h5>
<button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span>
</button>
</div>
<div class=modal-body>
<pre><code class="tex hljs"></code></pre>
</div>
<div class=modal-footer>
<a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank>
<i class="fas fa-copy"></i> Copy
</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank>
<i class="fas fa-download"></i> Download
</a>
<div id=modal-error></div>
</div>
</div>
</div>
</div>
<script src=/js/wowchemy-publication.ee00aa4e09ee62617fe2dc15bfcb3f7b.js type=module></script>
</body>
</html>